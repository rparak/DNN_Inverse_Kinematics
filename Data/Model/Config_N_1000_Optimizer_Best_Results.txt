use_bias: 1
hidden_layer_activation: relu
layer_dropout: 0.05
in_layer_activation: tanh
num_of_hidden_layers: 3
hidden_layer_1_units: 64
out_layer_activation: tanh
learning_rate: 0.01
hidden_layer_2_units: 128
hidden_layer_3_units: 64
